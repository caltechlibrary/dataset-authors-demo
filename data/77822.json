{"title":"Generating Long-term Trajectories Using Deep Hierarchical Networks","uri":"","abstract":"We study the problem of modeling spatiotemporal trajectories over long time horizons using expert demonstrations. For instance, in sports, agents often choose action sequences with long-term goals in mind, such as achieving a certain strategic position. Conventional policy learning approaches, such as those based on Markov decision processes, generally fail at learning cohesive long-term behavior in such high-dimensional state spaces, and are only effective when fairly myopic decision-making yields the desired behavior. The key difficulty is that conventional models are “single-scale” and only learn a single state-action policy. We instead propose a hierarchical policy class that automatically reasons about both long-term and short-term goals, which we instantiate as a hierarchical neural network. We showcase our approach in a case study on learning to imitate demonstrated basketball trajectories, and show that it generates significantly more realistic trajectories compared to non-hierarchical baselines as judged by professional sports analysts.","documents":[{"id":"http://authors.library.caltech.edu/id/document/252864","docid":252864,"rev_number":3,"files":[{"id":"http://authors.library.caltech.edu/id/file/1264289","fileid":1264289,"datasetid":"document","objectid":252864,"filename":"6520-generating-long-term-trajectories-using-deep-hierarchical-networks.pdf","mime_type":"application/pdf","hash":"fa89e72f8fc87328cdfe678867875542","hash_type":"MD5","filesize":958839,"mtime":"2017-05-30 16:06:02","url":"http://authors.library.caltech.edu/77822/1/6520-generating-long-term-trajectories-using-deep-hierarchical-networks.pdf"}],"eprintid":77822,"pos":1,"placement":1,"mime_type":"application/pdf","format":"application/pdf","language":"en","security":"public","license":"other","main":"6520-generating-long-term-trajectories-using-deep-hierarchical-networks.pdf","content":"published"},{"id":"http://authors.library.caltech.edu/id/document/252865","docid":252865,"rev_number":3,"files":[{"id":"http://authors.library.caltech.edu/id/file/1264292","fileid":1264292,"datasetid":"document","objectid":252865,"filename":"6520-generating-long-term-trajectories-using-deep-hierarchical-networks-supplemental.zip","mime_type":"application/zip","hash":"7d9ebd33f314ab59410191d7a1e51e1c","hash_type":"MD5","filesize":1831682,"mtime":"2017-05-30 16:06:15","url":"http://authors.library.caltech.edu/77822/2/6520-generating-long-term-trajectories-using-deep-hierarchical-networks-supplemental.zip"}],"eprintid":77822,"pos":2,"placement":2,"mime_type":"application/zip","format":"application/zip","language":"en","security":"public","license":"other","main":"6520-generating-long-term-trajectories-using-deep-hierarchical-networks-supplemental.zip","content":"supplemental"}],"note":"© 2016 Neural Information Processing Systems Foundation, Inc. \n\nThis research was supported in part by NSF Award #1564330, and a GPU donation (Tesla K40 and Titan X) by NVIDIA.","id":77822,"rev_number":14,"userid":6,"eprint_dir":"disk0/00/07/78/22","datestamp":"2017-05-30 17:16:19","lastmod":"2017-05-30 17:16:19","status_changed":"2017-05-30 17:16:19","type":"book_section","metadata_visibility":"show","creators":[{"given":"Stephan","family":"Zheng","id":"Zheng-Stephan","orcid":""},{"given":"Yisong","family":"Yue","id":"Yue-Yisong","orcid":""},{"given":"Patrick","family":"Lucey","id":"Lucey-P","orcid":""}],"ispublished":"pub","subjects":null,"full_text_status":"public","keywords":"","date":"2016-12","date_type":"published","publication":"","volume":"3","number":"","pagerange":"1551-1559","id_number":"CaltechAUTHORS:20170530-090151984","refereed":true,"issn":"","official_url":"http://resolver.caltech.edu/CaltechAUTHORS:20170530-090151984","related_url":[{"url":"http://papers.nips.cc/paper/6520-generating-long-term-trajectories-using-deep-hierarchical-networks","type":"pub","description":"Article"}],"referencetext":["[1] Aijun Bai, Feng Wu, and Xiaoping Chen. Online planning for large markov decision processes with\nhierarchical decomposition. ACM Transactions on Intelligent Systems and Technology (TIST), 6(4):45,\n2015.\n[2] Richard W Byrne and Anne E Russon. Learning by imitation: A hierarchical approach. Behavioral and\nbrain sciences, 21(05):667–684, 1998.\n[3] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback recurrent neural\nnetworks. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille,\nFrance, 6-11 July 2015, pages 2067–2075, 2015.\n[4] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A\nrecurrent latent variable model for sequential data. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,\nand R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2980–2988. Curran\nAssociates, Inc., 2015.\n[5] Jonathan St B. T. Evans and Keith E. Stanovich. Dual-Process Theories of Higher Cognition Advancing the\nDebate. Perspectives on Psychological Science, 8(3):223–241, May 2013. ISSN 1745-6916, 1745-6924.\ndoi: 10.1177/1745691612460685.\n[6] Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efficient Solution Algorithms\nfor Factored MDPs. J. Artif. Int. Res., 19(1):399–468, October 2003. ISSN 1076-9757.\n[7] Matthew Hausknecht and Peter Stone. Deep reinforcement learning in parameterized action space. In\nProceedings of the International Conference on Learning Representations (ICLR), San Juan, Puerto Rico,\nMay .\n[8] Ruijie He, Emma Brunskill, and Nicholas Roy. PUMA: Planning Under Uncertainty with Macro-Actions.\nIn Twenty-Fourth AAAI Conference on Artificial Intelligence, July 2010.\n[9] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift. pages 448–456, 2015.\n[10] George Konidaris, Scott Kuindersma, Roderic Grupen, and Andrew Barto. Robot learning from demonstration\nby constructing skill trees. The International Journal of Robotics Research, 31(3):360–375, March\n2012. ISSN 0278-3649, 1741-3176. doi: 10.1177/0278364911428653.\n[11] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie,\nAmir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and\nDemis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533,\nFebruary 2015. ISSN 0028-0836. doi: 10.1038/nature14236.\n[12] Katharina Muelling, Abdeslam Boularias, Betty Mohler, Bernhard Schölkopf, and Jan Peters. Learning\nstrategies in table tennis using inverse reinforcement learning. Biological Cybernetics, 108(5):603–619,\nOctober 2014. ISSN 1432-0770. doi: 10.1007/s00422-014-0599-1.\n[13] Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Autonomous Agents\nand Multi-Agent Systems, 11(3):387–434, 2005.\n[14] Richard S. Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A framework for\ntemporal abstraction in reinforcement learning. Artificial Intelligence, 112(1–2):181–211, August 1999.\nISSN 0004-3702. doi: 10.1016/S0004-3702(99)00052-1.\n[15] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard\nZemel, and Yoshua Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual\nAttention. arXiv:1502.03044 [cs], February 2015. arXiv: 1502.03044.\n[16] Yisong Yue, Patrick Lucey, Peter Carr, Alina Bialkowski, and Iain Matthews. Learning Fine-Grained\nSpatial Models for Dynamic Sports Play Prediction. In IEEE International Conference on Data Mining\n(ICDM).\n[17] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse\nreinforcement learning. In AAAI, pages 1433–1438, 2008."],"rights":"No commercial reproduction, distribution, display or performance rights in this work are provided.","official_citation":"","other_numbering_system":null,"funders":[{"agency":"NSF","grant_number":"IIS-1564330"}],"collection":"CaltechAUTHORS","reviewer":"","local_group":null}