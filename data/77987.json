{"title":"Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review","uri":"","abstract":"The paper reviews and extends an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. A class of deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Implications of a few key theorems are discussed, together with new results, open problems and conjectures.","documents":[{"id":"http://authors.library.caltech.edu/id/document/253301","docid":253301,"rev_number":4,"files":[{"id":"http://authors.library.caltech.edu/id/file/1267544","fileid":1267544,"datasetid":"document","objectid":253301,"filename":"art%3A10.1007%2Fs11633-017-1054-2.pdf","mime_type":"application/pdf","hash":"516d85e2e44887a35fa795cbc69a11aa","hash_type":"MD5","filesize":1756837,"mtime":"2017-06-07 14:30:23","url":"http://authors.library.caltech.edu/77987/1/art%253A10.1007%252Fs11633-017-1054-2.pdf"}],"eprintid":77987,"pos":1,"placement":1,"mime_type":"application/pdf","format":"application/pdf","language":"en","security":"public","license":"cc_by","main":"art%3A10.1007%2Fs11633-017-1054-2.pdf","content":"inpress"},{"id":"http://authors.library.caltech.edu/id/document/253324","docid":253324,"rev_number":3,"files":[{"id":"http://authors.library.caltech.edu/id/file/1267695","fileid":1267695,"datasetid":"document","objectid":253324,"filename":"1611.00740","mime_type":"application/pdf","hash":"ff55635c83d58dfe55f04f09078ddf0d","hash_type":"MD5","filesize":2568559,"mtime":"2017-06-07 16:14:09","url":"http://authors.library.caltech.edu/77987/2/1611.00740"}],"eprintid":77987,"pos":2,"placement":2,"mime_type":"application/pdf","format":"application/pdf","language":"en","security":"public","license":"other","main":"1611.00740","content":"submitted"}],"note":"© 2017 The Author(s). This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. \n\nManuscript received November 3, 2016; accepted December 12, 2017. \n\nRecommended by Associate Editor Hong Qiao. \n\nSpecial Issue on Human Inspired Computing. \n\nThis work was supported by the Center for Brains, Minds and Machines (CBMM), NSF STC award CCF (No. 1231216), and ARO (No.W911NF-15-1-0385). \n\nThe authors thank O. Shamir for useful emails that prompted us to clarify our results in the context of lower bounds.","id":77987,"rev_number":11,"userid":772,"eprint_dir":"disk0/00/07/79/87","datestamp":"2017-06-07 16:14:19","lastmod":"2017-06-07 16:15:54","status_changed":"2017-06-07 16:14:19","type":"article","metadata_visibility":"show","creators":[{"given":"Tomaso","family":"Poggio","id":"Poggio-T","orcid":""},{"given":"Hrushikesh N.","family":"Mhaskar","id":"Mhaskar-H-N","orcid":""},{"given":"Lorenzo","family":"Rosasco","id":"Rosasco-L","orcid":""},{"given":"Brando","family":"Miranda","id":"Miranda-B","orcid":""},{"given":"Qianli","family":"Liao","id":"Liao-Qianli","orcid":""}],"ispublished":"inpress","subjects":null,"full_text_status":"public","keywords":"Machine learning neural networks deep and shallow networks convolutional neural networks function approximation deep learning ","date":"2017-03-14","date_type":"inpress","publication":"International Journal of Automation and Computing","volume":"","number":"","pagerange":"","id_number":"CaltechAUTHORS:20170607-072954485","refereed":true,"issn":"1476-8186","official_url":"http://resolver.caltech.edu/CaltechAUTHORS:20170607-072954485","related_url":[{"url":"http://dx.doi.org/10.1007/s11633-017-1054-2","type":"doi","description":"Article"},{"url":"https://link.springer.com/article/10.1007/s11633-017-1054-2","type":"pub","description":"Article"},{"url":"http://rdcu.be/tirF","type":"pub","description":"Free ReadCube access"},{"url":"https://arxiv.org/abs/1611.00740","type":"arxiv","description":"Discussion Paper"}],"referencetext":["[1] F. Anselmi, L. Rosasco, C. Tan, T. Poggio. Deep Convolutional\nNetworks are Hierarchical Kernel Machines, Center\nfor Brains, Minds and Machines (CBMM) Memo No. 035,\nThe Center for Brains, Minds and Machines, USA, 2015.\n[2] T. Poggio, L. Rosasco, A. Shashua, N. Cohen, F. Anselmi.\nNotes on Hierarchical Splines, DCLNs and i-theory, Center\nfor Brains, Minds and Machines (CBMM) Memo No. 037,\nThe Center for Brains, Minds and Machines, USA, 2015.\n[3] T. Poggio, F. Anselmi, L. Rosasco. I-theory on Depth\nvs Width: Hierarchical Function Composition, Center for\nBrains, Minds and Machines (CBMM) Memo No. 041, The\nCenter for Brains, Minds and Machines, USA, 2015.\n[4] H. Mhaskar, Q. L. Liao, T. Poggio. Learning Real and\nBoolean Functions: When is Deep Better than Shallow,\nCenter for Brains, Minds and Machines (CBMM) Memo\nNo. 045, The Center for Brains, Minds and Machines, USA,\n2016.\n[5] H. N. Mhaskar, T. Poggio. Deep Vs. Shallow Networks:\nAn Approximation Theory Perspective, Center for Brains,\nMinds and Machines (CBMM) Memo No. 054, The Center\nfor Brains, Minds and Machines, USA, 2016.\n[6] D. L. Donoho. High-dimensional data analysis: The curses\nand blessings of dimensionality. Lecture – Math Challenges\nof Century, vol. 13, pp. 178–183, 2000.\n[7] Y. LeCun, Y. Bengio, G. Hinton. Deep learning. Nature,\nvol. 521, no. 7553, pp. 436–444, 2015.\n[8] K. Fukushima. Neocognitron: A self-organizing neural network\nmodel for a mechanism of pattern recognition unaffected\nby shift in position. Biological Cybernetics, vol. 36,\nno. 4, pp. 193–202, 1980.\n[9] M. Riesenhuber, T. Poggio. Hierarchical models of object\nrecognition in cortex. Nature Neuroscience, vol. 2, no. 11,\npp. 1019–1025, 1999.\n[10] H. N. Mhaskar. Approximation properties of a multilayered\nfeedforward artificial neural network. Advances in Computational\nMathematics, vol. 1, no. 1, pp. 61–80, 1993.\n[11] C. K. Chui, X. Li, H. Mhaskar. Neural networks for localized\napproximation. Mathematics of Computation, vol. 63,\nno. 208, pp. 607–623, 1994.\n[12] C. K. Chui, X. Li, H. N. Mhaskar. Limitations of the approximation\ncapabilities of neural networks with one hidden\nlayer. Advances in Computational Mathematics, vol.5,\nno. 1, pp. 233–243, 1996.\n[13] A. Pinkus. Approximation theory of the MLP model in neural\nnetworks. Acta Numerica, vol. 8, pp. 143–195, 1999.\n[14] T. Poggio, S. Smale. The mathematics of learning: Dealing\nwith data. Notices of the American Mathematical Society,\nvol. 50, no. 5, pp. 537–544, 2003.\n[15] B. Moore, T. Poggio. Representation properties of multilayer\nfeedforward networks. Neural Networks, vol.1, no.S1,\npp. 203, 1998.\n[16] R. Livni, S. Shalev-Shwartz, O. Shamir. A provably\nefficient algorithm for training deep networks. CoRR,\nabs/1304.7045, 2013.\n[17] O. Delalleau, Y. Bengio. Shallow vs. deep sum-product networks.\nIn Proceedings of Advances in Neural Information\nProcessing Systems 24, NIPS, Granada, Spain, pp. 666–674,\n2011.\n[18] G. F. Montufar, R. Pascanu, K. Cho, Y. Bengio. On the\nnumber of linear regions of deep neural networks. In Proceedings\nof Advances in Neural Information Processing Systems\n27, NIPS, Denver, USA, pp. 2924–2932, 2014.\n[19] H. N. Mhaskar. Neural networks for localized approximation\nof real functions. In Proceedings of IEEE-SPWorkshop\non Neural Networks for Processing III, pp. 190–196, IEEE,\nLinthicum Heights, USA, 1993.\n[20] N. Cohen, O. Sharir, A. Shashua. On the expressive power\nof deep learning: A tensor analysis. arXiv:1509.0500v1,\n2015.\n[21] F. Anselmi, J. Z. Leibo, L. Rosasco, J. Mutch, A. Tacchetti,\nT. Poggio. Unsupervised Learning of Invariant Representations\nWith Low Sample Complexity: The Magic of Sensory\nCortex or A New Framework for Machine Learning? Center\nfor Brains, Minds and Machines (CBMM) Memo No. 001,\nThe Center for Brains, Minds and Machines, USA, 2014.\n[22] F. Anselmi, J. Z. Leibo, L. Rosasco, J. Mutch, A. Tacchetti,\nT. Poggio. Unsupervised learning of invariant representations.\nTheoretical Computer Science, vol. 633, pp. 112–121,\n2016.\n[23] T. Poggio, L. Rosaco, A. Shashua, N. Cohen, F. Anselmi.\nNotes on Hierarchical Splines, DCLNs and i-theory, Center\nfor Brains, Minds and Machines (CBMM) Memo No. 037.\nThe Center for Brains, Minds and Machines, 2015.\n[24] Q. L. Liao, T. Poggio. Bridging the Gaps between Residual\nLearning, Recurrent Neural Networks and Visual Cortex,\nCenter for Brains, Minds and Machines (CBMM) Memo\nNo. 047, The Center for Brains, Minds and Machines, 2016.\n[25] M. Telgarsky. Representation benefits of deep feedforward\nnetworks. arXiv:1509.08101v2, 2015.\n[26] I. Safran, O. Shamir. Depth separation in ReLU networks\nfor approximating smooth non-linear functions.\narXiv:1610.09887v1, 2016.\n[27] H. N. Mhaskar. Neural networks for optimal approximation\nof smooth and analytic functions. Neural Computation,\nvol. 8, no. 1, pp. 164–177, 1996.\n[28] E. Corominas, F. S. Balaguer. Conditions for an infinitely\ndifferentiable function to be a polynomial. Revista\nMatem´atica Hispanoamericana vol. 14, no. 1–2, pp. 26–43,\n1954. (in Spanish)\n[29] T. Poggio, H. Mhaskar, L. Rosasco, B. Miranda, Q. L. Liao.\nWhy and when can deep–but not shallow–networks avoid\nthe curse of dimensionality: A review. arXiv:1611.00740v3,\n2016.\n[30] R. A. DeVore, R. Howard C. A. Micchelli. Optimal nonlinear\napproximation. Manuscripta Mathematica, vol. 63,\nno. 4, pp. 469–478, 1989.\n[31] H. N. Mhaskar. On the tractability of multivariate integration\nand approximation by neural networks. Journal of\nComplexity, vol. 20, no. 4, pp. 561–590, 2004.\n[32] F. Bach. Breaking the curse of dimensionality with convex\nneural networks. arXiv:1412.8690, 2014.\n[33] D. Kingma, J. Ba. Adam: A method for stochastic optimization.\narXiv:1412.6980, 2014.\n[34] J. Bergstra, Y. Bengio. Random search for hyper-parameter\noptimization. Journal of Machine Learning Research,\nvol. 13, no. 1, pp. 281–305, 2012.\n[35] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. F. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S.\nGhemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard,\nY. Q. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg,\nD. Man´e, R. Monga, S. Moore, D. Murray, C. Olah,\nM. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar,\nP. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas,\nO. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu,\nX. Q. Zheng. TensorFlow: Large-scale machine learning on\nheterogeneous distributed systems. arXiv:1603.04467, 2016.\n[36] R. Eldan, O. Shamir. The power of depth for feedforward\nneural networks. arXiv:1512.03965v4, 2016.\n[37] H. W. Lin, M. Tegmark. Why does deep and cheap learning\nwork so well? arXiv:1608.08225, 2016.\n[38] J. T. H˚astad. Computational Limitations for Small Depth\nCircuits, Cambridge, MA, USA: MIT Press, 1987.\n[39] N. Linial, Y. Mansour, N. Nisan. Constant depth circuits,\nFourier transform, and learnability. Journal of the ACM,\nvol. 40, no. 3, pp. 607–620, 1993.\n[40] Y. Bengio, Y. LeCun. Scaling learning algorithms towards\nAI. Large-Scale Kernel Machines, L. Bottou, O. Chapelle,\nD. DeCoste, J. Weston, Eds., Cambridge, MA, USA: MIT\nPress, 2007.\n[41] Y. Mansour. Learning Boolean functions via the Fourier\ntransform. Theoretical Advances in Neural Computation\nand Learning, V. Roychowdhury, K. Y. Siu, A. Orlitsky,\nEds., pp. 391–424, US: Springer, 1994.\n[42] M. Anthony, P. Bartlett. Neural Network Learning: Theoretical\nFoundations, Cambridge, UK: Cambridge University\nPress, 2002.\n[43] F. Anselmi, L. Rosasco, C. Tan, T. Poggio. Deep Convolutional\nNetworks are Hierarchical Kernel Machines, Center\nfor Brains, Minds and Machines (CBMM) Memo No. 035,\nThe Center for Brains, Minds and Machines, USA, 2015.\n[44] B. M. Lake, R. Salakhutdinov, J. B. Tenenabum. Humanlevel\nconcept learning through probabilistic program induction.\nScience, vol. 350, no. 6266, pp. 1332–1338, 2015.\n[45] A. Maurer. Bounds for linear multi-task learning. Journal of\nMachine Learning Research, vol. 7, no. 1, pp. 117–139, 2016.\n[46] S. Soatto. Steps towards a theory of visual information: Active\nperception, signal-to-symbol conversion and the interplay\nbetween sensing and control. arXiv:1110.2053, 2011.\n[47] T. A. Poggio, F. Anselmi. Visual Cortex and Deep Networks:\nLearning Invariant Representations, Cambridge,\nMA, UK: MIT Press, 2016.\n[48] L. Grasedyck. Hierarchical singular value decomposition of\ntensors. SIAM Journal on Matrix Analysis and Applications,\nno. 31, no. 4, pp. 2029–2054, 2010.\n[49] S. Shalev-Shwartz, S. Ben-David. Understanding Machine\nLearning: From Theory to Algorithms, Cambridge, UK:\nCambridge University Press, 2014.\n[50] T. Poggio, W. Reichardt. On the representation of multiinput\nsystems: Computational properties of polynomial\nalgorithms. Biological Cybernetics, vol. 37, no. 3, 167-186,\n1980.\n[51] M. L. Minsky, S. A. Papert. Perceptrons: An Introduction\nto Computational Geometry, Cambridge MA, UK: The\nMIT Press, 1972"],"rights":"No commercial reproduction, distribution, display or performance rights in this work are provided.","official_citation":"Poggio, T., Mhaskar, H., Rosasco, L. et al. Int. J. Autom. Comput. (2017). doi:10.1007/s11633-017-1054-2","other_numbering_system":null,"funders":[{"agency":"Center for Brains, Minds and Machines (CBMM)","grant_number":""},{"agency":"NSF","grant_number":"CCF-1231216"},{"agency":"Army Research Office (ARO)","grant_number":"W911NF-15-1-0385"}],"collection":"CaltechAUTHORS","reviewer":"George Porter","local_group":null}